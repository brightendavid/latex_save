@article{Vinyals2014,
  title={Show and Tell: A Neural Image Caption Generator},
  author={Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
  year={2014},
  journal={arXiv preprint arXiv:1411.4555v2},
}
@article{Song2019,
  title={Time-series well performance prediction based on Long Short-Term Memory (LSTM) neural network model},
  author={Song, Xuanyi and Liu, Yuetian and Xue, Liang and Wang, Jun and Zhang, Jingzhe and Wang, Junqiang and Jiang, Long and Cheng, Ziyan},
  year={2019},
  journal={Journal of Petroleum Science and Engineering},
  volume={186},
  pages={106682-106682},
  doi={https://doi.org/10.1016/j.petrol.2019.106682},
}
@article{Bianchi2021,
  title={Contrastive Language-Image Pre-training for the Italian Language},
  author={Bianchi, Federico and Attanasio, Giuseppe and Pisoni, Raphael and Terragni, Silvia and Sarti, Gabriele and Lakshmi, Sri},
  year={2021},
  journal={arXiv (Cornell University)},
  doi={https://doi.org/10.48550/arxiv.2108.08688},
}

@inproceedings{b389343f08b34314ae257cae694352e6,
title = "How Many Words Is a Picture Worth? Automatic Caption Generation for News Images",
abstract = "In this paper we tackle the problem of automatic caption generation for news images. Our approach leverages the vast resource of pictures available on the web and the fact that many of them are captioned. Inspired by recent work in summarization, we propose extractive and abstractive caption generation models. They both operate over the output of a probabilistic image annotation model that pre-processes the pictures and suggests key-words to describe their content. Experimental results show that an abstractive model defined over phrases is superior to extractive methods.",
author = "Yansong Feng and Mirella Lapata",
year = "2010",
language = "English",
pages = "1239--1249",
booktitle = "ACL 2010, Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, July 11-16, 2010, Uppsala, Sweden",
publisher = "Association for Computational Linguistics",
}
@InProceedings{Vinyals_2015_CVPR,
author = {Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
title = {Show and Tell: A Neural Image Caption Generator},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2015}
}
@article{Xu2015,
  title={Show, Attend and Tell: Neural Image Caption Generation with Visual Attention},
  author={Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard and Bengio, Yoshua},
  year={2015},
  journal={arXiv (Cornell University)},
  doi={https://doi.org/10.48550/arxiv.1502.03044},
}
@article{Chen2017,
  title={SCA-CNN: Spatial and Channel-Wise Attention in Convolutional Networks for Image Captioning},
  author={Chen, Long and Zhang, Hanwang and Xiao, Jun and Nie, Liqiang and Shao, Jian and Liu, Wei and Chua, Tat‐Seng},
  year={2017},
  journal={arXiv preprint https://arxiv.org/pdf/1611.05594},
  doi={https://doi.org/10.1109/cvpr.2017.667},
}
@article{Yao2018,
  title={Exploring Visual Relationship for Image Captioning},
  author={Yao, Ting and Pan, Yingwei and Li, Yehao and Mei, Tao},
  year={2018},
  journal={Lecture notes in computer science},
  pages={711-727},
  doi={https://doi.org/10.1007/978-3-030-01264-9_42},
}
@article{Zhou2020,
  title={Unified Vision-Language Pre-Training for Image Captioning and VQA},
  author={Zhou, Luowei and Palangi, Hamid and Zhang, Lei and Hu, Houdong and Corso, Jason and Gao, Jianfeng},
  year={2020},
  journal={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={07},
  pages={13041-13049},
  doi={https://doi.org/10.1609/aaai.v34i07.7005},
}
@article{Alaparthi2020,
  title={Bidirectional Encoder Representations from Transformers (BERT): A sentiment analysis odyssey},
  author={Alaparthi, Shivaji and Mishra, Manit},
  year={2020},
  journal={arXiv (Cornell University)},
  doi={https://doi.org/10.48550/arxiv.2007.01127},
}
@InProceedings{pmlr-v139-radford21a,
  title = 	 {Learning Transferable Visual Models From Natural Language Supervision},
  author =       {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8748--8763},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/radford21a/radford21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/radford21a.html},
  abstract = 	 {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.}
}
@article{Rombach2021,
  title={High-Resolution Image Synthesis with Latent Diffusion Models},
  author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Björn},
  year={2021},
  journal={arXiv (Cornell University)},
  doi={https://doi.org/10.48550/arxiv.2112.10752},
}
@article{Li2022,
  title={BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation},
  author={Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
  year={2022},
  journal={arXiv (Cornell University)},
  doi={https://doi.org/10.48550/arxiv.2201.12086},
}
@inproceedings{papineni2002bleu,
  title={BLEU: A method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th Annual Meeting on Association for Computational Linguistics},
  pages={311--318},
  year={2002},
  organization={Association for Computational Linguistics}
}
@article{Lavie2009,
  title={The Meteor metric for automatic evaluation of machine translation},
  author={Lavie, Alon and Denkowski, Michael},
  year={2009},
  journal={Machine Translation},
  volume={23},
  number={2-3},
  pages={105-115},
  doi={https://doi.org/10.1007/s10590-009-9059-4},
}
@article{Vedantam2014,
  title={CIDEr: Consensus-based Image Description Evaluation},
  author={Vedantam, Ramakrishna and Zitnick, Lawrence and Parikh, Devi},
  year={2014},
  journal={arXiv (Cornell University)},
  doi={https://doi.org/10.48550/arxiv.1411.5726},
}
@article{Lin2024,
  title={Microsoft COCO: common objects in context},
  author={Lin, Tsung-Yi},
  year={2024},
  journal={TIB Data Manager},
  doi={https://doi.org/10.57702/lydv1ylk},
}
@article{Plummer2016,
  title={Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models},
  author={Plummer, Bryan and Wang, Liwei and Cervantes, Chris and Caicedo, Juan and Hockenmaier, Julia and Lazebnik, Svetlana and Plummer, Bryan and Wang, Liwei and Cervantes, Chris and Caicedo, Juan and Hockenmaier, Julia and Lazebnik, Svetlana},
  year={2016},
  journal={International Journal of Computer Vision},
  volume={123},
  number={1},
  pages={74-93},
  doi={https://doi.org/10.1007/s11263-016-0965-7},
}
@article{Krishna2017,
  title={Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations},
  author={Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David and Bernstein, Michael and Fei-Fei, Li and Shamma, David},
  year={2017},
  journal={International Journal of Computer Vision},
  volume={123},
  number={1},
  pages={32-73},
  doi={https://doi.org/10.1007/s11263-016-0981-7},
}
