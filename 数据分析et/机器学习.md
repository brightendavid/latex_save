# 机器学习

寻找一个函数

杂乱数据中，找到规律

## 参考书

* 机器学习
* 机器学习实战
* 深度学习

## 要素

模型，策略，算法



## 强化学习

 智能体（agent）

* 人工智能：一种现代的方法

交互

核心：试错，后果

通过评分完成

通向通用人工智能

求解和场景复杂度无关

state,agent,q值 max,

状态评估表（策略表），Q表（所有状态，所有动作，取分最高的动作，(s,a)）

策略：在s下作什么a动作，a= p(s)

价值：收益



## 分类

有模型(基于模型推演)

* Dyna
* 蒙特卡洛树

无模型（主流，直接通过试错来学习策略或价值函数

 * 基于价值
   *  ** q-learn **
   * dqn
 * 基于策略
   * 策略梯度
   * 深度策略梯度
 * AC***\*演员-评论员\****

混合方法结合：ppo



# 深度学习

* 李航，机器学习

* 机器学习实战



# 理论部分

forbidden可以进入，可计算才是强化学习，向量化-》可编程

> forbidden:-1
>
> target :1
>
> 边框：-1
>
> 正常框：0
>
> state：agent在环境中的当前状态，位置
>
> s6,s7：forbidden
>
> State Space：状态空间
>
> action:a1,a2,a3,a4,a5:上，右，下，左，不动
>
> A（s），A是状态s的函数
>
> state transition ：s状态的变化
>
> state transition probabilty
>
> policy:告诉Agent在某一state时应该采取什么action
>
> trajectory:路径的回顾，path,chain
>
> π：策略描述，π（a1|s1）=0，映射，在s1采取动作a1的概率
>
> stochastic policy：随机（不确定）
>
> reward。重要概念r。是一个实数，是标量，具体的数据，有一定的含义，正数则为鼓励，负数则是代表惩罚。
>
> reward is human-machine interface
>
> return:reward相加
>
> ***\*Episode。有限步骤\****
>
> Discounted return：无限步骤，加gama 算return=0+gama*0+... gama^i * reward
>
> state transition
>
> reward probability
>
> Markov propety: memeoryless property：
>
> 马尔可夫Markov decision process
>
> on policy
>
> off policy



p(s6|s3,a3)=1

p(si|s3,a3)=0 i!=6

# 贝尔曼公式

> * Bootstrapping迭代
>
> * v=r+γPv  ，v：state value
> * vΠ（s）最终的reward
> * return和state value之间的区别是什么？return针对单个trajectory，state value是多个trajectory求平均。
> * qπ（s,a）：action value



1. > Optimal state value：最优值
   >
   > 找到最优的π
   >

# VFA

v hat is 近似v

v hat(a,w) = as +b = ( s ,  1  )[a ,b]^T = φ^T (s) * w



=  (s^2,s,1)[a,b,c]^T



# PGM

